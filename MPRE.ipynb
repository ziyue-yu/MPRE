{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, using SCRIPT CarpeDiem Dataset as example\n",
    "df = pd.read_csv('...Data root...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static features and dynamic features and labels\n",
    "\n",
    "df_static = df[['Patient_id','Age', 'Ethnicity','Gender','Race','Smoking_status','BMI','Admit_APS_score','Admit_SOFA_score']]\n",
    "df_dynamic = df[['Patient_id','SOFA_score', 'Temperature',\n",
    "       'Heart_rate', 'Systolic_blood_pressure', 'Diastolic_blood_pressure',\n",
    "       'Mean_arterial_pressure', 'Respiratory_rate', 'Oxygen_saturation',\n",
    "       'Urine_output', 'PEEP', 'FiO2', 'Plateau_Pressure', 'Lung_Compliance',\n",
    "       'PEEP_changes', 'Respiratory_rate_changes', 'FiO2_changes', 'WBC_count',\n",
    "       'Lymphocytes', 'Neutrophils', 'Hemoglobin', 'Platelets', 'Bicarbonate',\n",
    "       'Creatinine', 'Albumin', 'Bilirubin', 'Procalcitonin']]\n",
    "y_label = df[['Patient_id','Patient_category']]\n",
    "\n",
    "\n",
    "df_static_result = df_static.drop_duplicates(subset='Patient_id', keep='first')\n",
    "# select multi feature as new dataframe\n",
    "multi = df_static_result[['Ethnicity', 'Gender', 'Race','Smoking_status']]\n",
    "multi_nominal = multi.to_numpy()\n",
    "multi_one_hot = MultiLabelBinarizer()\n",
    "static_one_hot = multi_one_hot.fit_transform(multi_nominal)\n",
    "numer_stat = df_static_result[['Age','BMI', 'Admit_APS_score', 'Admit_SOFA_score']]\n",
    "multi_numer = numer_stat.to_numpy()\n",
    "numericalOfStatic = preprocessing.normalize(multi_numer, norm='l2')\n",
    "static_feature = np.concatenate((static_one_hot, numericalOfStatic), axis=1)\n",
    "\n",
    "y_label_result = y_label.drop_duplicates(subset='Patient_id',keep='first')\n",
    "enc = preprocessing.LabelEncoder()\n",
    "y_label_final = enc.fit_transform(y_label_result['Patient_category'])\n",
    "\n",
    "\n",
    "\n",
    "cols_to_normalize = df_dynamic.columns[1:]\n",
    "\n",
    "\n",
    "normalized_data = preprocessing.normalize(df_dynamic[cols_to_normalize], norm='l2')\n",
    "\n",
    "\n",
    "df_normalized = pd.concat([df_dynamic['Patient_id'], pd.DataFrame(normalized_data, columns=cols_to_normalize)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df_normalized.groupby('Patient_id').head(10)  \n",
    "\n",
    "features_data = [[] for _ in range(26)]  \n",
    "\n",
    "for idx, group in grouped_data.groupby('Patient_id'):  \n",
    "    for i in range(26):  \n",
    "        features_data[i].append(group.iloc[:10, i + 1].tolist())  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "processed_features_data = [[] for _ in range(26)]  \n",
    "\n",
    "for j in range(len(features_data)):  \n",
    "    for i in range(len(features_data[j])):  \n",
    "        wavelet = 'sym18'  \n",
    "        level = 1  \n",
    "        mode = 'symmetric'  \n",
    "        coeffs = pywt.wavedec(features_data[j][i], wavelet, level=level, mode=mode)  \n",
    "        processed_features_data[j].append(coeffs)  \n",
    "\n",
    "del processed_features_data[-5] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n"
     ]
    }
   ],
   "source": [
    "# Low frequency component (trend information)  \n",
    "def trend_information(coe):  \n",
    "    return [c[0] for c in coe]  \n",
    "\n",
    "trend_info_list = [trend_information(feature) for feature in processed_features_data]  \n",
    "\n",
    "\n",
    "cA_a, cA_b, cA_c, cA_d, cA_e, cA_f, cA_g, cA_h, cA_I, \\\n",
    "cA_j, cA_k, cA_l, cA_m, cA_n, cA_o, cA_p, cA_q, cA_r, \\\n",
    "cA_s, cA_t, cA_u, cA_w, cA_xx, cA_yy, cA_zz = trend_info_list  \n",
    "\n",
    "\n",
    "\n",
    "print(len(cA_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def high_information(coe):  \n",
    "    return [c[1:] for c in coe]  \n",
    "\n",
    "processed_high_data = [high_information(feature) for feature in processed_features_data]  \n",
    "\n",
    " \n",
    "processed_high_data_extracted = [[sublist[0] for sublist in feature] for feature in processed_high_data]  \n",
    "\n",
    " \n",
    "cD3_a, cD3_b, cD3_c, cD3_d, cD3_e, cD3_f, cD3_g, cD3_h, cD3_I, \\\n",
    "cD3_j, cD3_k, cD3_l, cD3_m, cD3_n, cD3_o, cD3_p, cD3_q, cD3_r, \\\n",
    "cD3_s, cD3_t, cD3_u, cD3_w, cD3_xx, cD3_yy, cD3_zz = processed_high_data_extracted  \n",
    "\n",
    " \n",
    "print(len(cD3_a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295, 525)\n"
     ]
    }
   ],
   "source": [
    "def first_order_difference(high_information_list):  \n",
    "     \n",
    "    return [  \n",
    "        [high_information[i+1] - high_information[i] for i in range(len(high_information) - 1)]  \n",
    "        for high_information in high_information_list  \n",
    "    ]  \n",
    "\n",
    "\n",
    "high_data_list = [  \n",
    "    cD3_a, cD3_b, cD3_c, cD3_d, cD3_e, cD3_f, cD3_g, cD3_h, cD3_I, cD3_j,   \n",
    "    cD3_k, cD3_l, cD3_m, cD3_n, cD3_o, cD3_p, cD3_q, cD3_r, cD3_s, cD3_t,   \n",
    "    cD3_u, cD3_w, cD3_xx, cD3_yy, cD3_zz  \n",
    "]  \n",
    "\n",
    " \n",
    "diff_high_data_list = [first_order_difference(data) for data in high_data_list]  \n",
    "\n",
    "\n",
    "diff_cat_all = np.concatenate([np.array(diff) for diff in diff_high_data_list], axis=1)  \n",
    "\n",
    "\n",
    "print(diff_cat_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295, 2, 550)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "\n",
    " \n",
    "cA_list = [  \n",
    "    cA_a, cA_b, cA_c, cA_d, cA_e, cA_f, cA_g, cA_h, cA_I, cA_j,   \n",
    "    cA_k, cA_l, cA_m, cA_n, cA_o, cA_p, cA_q, cA_r, cA_s, cA_t,   \n",
    "    cA_u, cA_w, cA_xx, cA_yy, cA_zz  \n",
    "]  \n",
    "\n",
    "cD3_list = [  \n",
    "    cD3_a, cD3_b, cD3_c, cD3_d, cD3_e, cD3_f, cD3_g, cD3_h, cD3_I, cD3_j,   \n",
    "    cD3_k, cD3_l, cD3_m, cD3_n, cD3_o, cD3_p, cD3_q, cD3_r, cD3_s, cD3_t,   \n",
    "    cD3_u, cD3_w, cD3_xx, cD3_yy, cD3_zz  \n",
    "]  \n",
    "\n",
    "  \n",
    "tensor_list = [np.stack([cA, cD3], axis=1) for cA, cD3 in zip(cA_list, cD3_list)]  \n",
    "\n",
    " \n",
    "tensor_all = np.concatenate(tensor_list, axis=2)  \n",
    "\n",
    " \n",
    "print(tensor_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dynamic_features, static_features, diff_cat_all, labels):\n",
    "        self.dynamic_features = dynamic_features\n",
    "        self.static_features = static_features\n",
    "\n",
    "        # new\n",
    "        self.diff_cat_all = diff_cat_all\n",
    "\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        dynamic_features = self.dynamic_features[index]\n",
    "        static_features = self.static_features[index]\n",
    "        \n",
    "        # new\n",
    "        \n",
    "        diff_cat_all = self.diff_cat_all[index]\n",
    "\n",
    "        \n",
    "        static_features = torch.tensor(static_features, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        labels = self.labels[index]\n",
    "\n",
    "        \n",
    "        sample = {'dynamic_features': dynamic_features, 'static_features': static_features, 'diff_cat_all': diff_cat_all, 'labels': labels}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dynamic_conv1 = nn.Conv2d(550, 16, kernel_size=1, stride=1, padding=0).double()\n",
    "        self.dynamic_conv2 = nn.Conv2d(550, 16, kernel_size=1, stride=1, padding=0, dilation=(1, 2)).double()\n",
    "        self.dynamic_conv3 = nn.Conv2d(550, 16, kernel_size=1, stride=1, padding=0, dilation=(1, 3)).double()\n",
    "        self.static_linear1 = nn.Linear(16, 8).float()\n",
    "        self.final_linear = nn.Linear(629, 4).float()\n",
    "        self.norm = nn.BatchNorm1d(64).double()\n",
    "        self.norm2 = nn.BatchNorm1d(32).double()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # changes with attention\n",
    "        self.diff_linear = nn.Linear(21, 21).float()\n",
    "        self.diff_attention = nn.Linear(525, 525).float()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, dynamic_feature, static_feature, diff_cat_all):\n",
    "        \n",
    "        dynamic_feature_groups = dynamic_feature.unfold(1, 2, 12)\n",
    "\n",
    "        \n",
    "        cat_results = []\n",
    "        for i in range(dynamic_feature_groups.shape[1]):\n",
    "            group = dynamic_feature_groups[:, i, :, :]\n",
    "            group = torch.unsqueeze(group, dim=3)  \n",
    "\n",
    "            conv1_output = self.dynamic_conv1(group)\n",
    "            conv2_output = self.dynamic_conv2(group)\n",
    "            conv3_output = self.dynamic_conv3(group)\n",
    "            cat_result = torch.cat([conv1_output, conv2_output, conv3_output], dim=1)\n",
    "            cat_results.append(cat_result)\n",
    "\n",
    "        #################################################\n",
    "        diff_cat_all_group = diff_cat_all.unfold(1, 21, 21)\n",
    "        weight = []\n",
    "        for j in range(diff_cat_all_group.shape[1]):\n",
    "            group_diff = diff_cat_all_group[:, j, :]\n",
    "            # group_diff = torch.unsqueeze(group_diff, dim=3)\n",
    "            group_1 = torch.softmax(self.diff_linear(group_diff).float(), dim=1).float()\n",
    "            weight.append(group_1)\n",
    "        #################################################\n",
    "\n",
    "        \n",
    "        cat_final = torch.cat(cat_results, dim=1)\n",
    "        cat_weight = torch.cat(weight, dim=1)\n",
    "\n",
    "        static_feature = static_feature.float()\n",
    "        \n",
    "        static_output = self.static_linear1(static_feature).float()\n",
    "\n",
    "        # write attention here\n",
    "        # weight = torch.softmax(self.diff_attention(diff_cat_all).float(), dim=1).float()\n",
    "\n",
    "\n",
    "        \n",
    "        combined_output = torch.cat([cat_final.view(cat_final.shape[0], -1), static_output, cat_weight], dim=1)\n",
    "        combined_output = combined_output.float()\n",
    "        final_output = self.final_linear(combined_output).float()\n",
    "\n",
    "    \n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(k, dataset, model, loss_fn, optimizer, num_epochs, batch_size):\n",
    "    \n",
    "    fold_size = len(dataset) // k\n",
    "    folds = [torch.utils.data.Subset(dataset, range(i*fold_size, (i+1)*fold_size)) for i in range(k)]\n",
    "\n",
    "    \n",
    "    aurocs = []\n",
    "    auprcs = []\n",
    "    min_sens_p = []\n",
    "    weight_sum = None \n",
    "\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f\"Fold {i+1}/{k}\")\n",
    "        \n",
    "        test_data = folds[i]\n",
    "        train_data = torch.utils.data.ConcatDataset([fold for j, fold in enumerate(folds) if j != i])\n",
    "        train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in train_loader:\n",
    "                dynamic_features = batch['dynamic_features']\n",
    "                static_features = batch['static_features']\n",
    "                # new\n",
    "                diff_cat_all = batch['diff_cat_all']\n",
    "                labels = batch['labels']\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                # new\n",
    "                outputs = model(dynamic_features, static_features, diff_cat_all)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dynamic_features = test_data.dataset.dynamic_features\n",
    "            static_features = test_data.dataset.static_features\n",
    "\n",
    "            # new\n",
    "            diff_cat_all = test_data.dataset.diff_cat_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            diff_cat_all_group = diff_cat_all.unfold(1, 21, 21)\n",
    "            weight = [] \n",
    "            for j in range(diff_cat_all_group.shape[1]):\n",
    "                group_diff = diff_cat_all_group[:, j, :]\n",
    "                group_1 = torch.softmax(model.diff_linear(group_diff).float(), dim=1).float()\n",
    "                weight.append(group_1)\n",
    "\n",
    "            weight = torch.cat(weight, dim=1)\n",
    "        \n",
    "            if weight_sum is None:\n",
    "                weight_sum = weight\n",
    "            else:\n",
    "                weight_sum += weight\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            labels = test_data.dataset.labels\n",
    "\n",
    "            outputs = model(dynamic_features, static_features, diff_cat_all)\n",
    "            probs = torch.softmax(outputs.clone().detach().requires_grad_(True), dim=1).cpu().numpy()\n",
    "           \n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            \n",
    "            labels = np.eye(len(np.unique(labels)))[labels.cpu().numpy()]\n",
    "\n",
    "            auroc = roc_auc_score(labels, probs, multi_class='ovo')\n",
    "            auprc = average_precision_score(labels, probs, average='weighted')\n",
    "\n",
    "            p, r, f, s = precision_recall_fscore_support(np.argmax(labels, axis=1), preds, average='weighted')\n",
    "            if s is not None and p is not None:\n",
    "                min_sens_p.append(np.min([s, p]))\n",
    "            else:\n",
    "                min_sens_p.append(0.0)\n",
    "            \n",
    "            \n",
    "            # min_sens_p.append(np.min([s, p]))\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: AUROC={auroc:.4f}, AUPRC={auprc:.4f}\")\n",
    "\n",
    "        aurocs.append(auroc)\n",
    "        auprcs.append(auprc)\n",
    "\n",
    "    # Mean AUROC and AUPRC\n",
    "    print(f\"Mean AUROC={np.mean(aurocs):.4f}, Mean AUPRC={np.mean(auprcs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n",
      "Epoch 600/600: AUROC=0.7804, AUPRC=0.6766\n",
      "Fold 2/10\n",
      "Epoch 600/600: AUROC=0.8231, AUPRC=0.7218\n",
      "Fold 3/10\n",
      "Epoch 600/600: AUROC=0.8583, AUPRC=0.7672\n",
      "Fold 4/10\n",
      "Epoch 600/600: AUROC=0.8739, AUPRC=0.7868\n",
      "Fold 5/10\n",
      "Epoch 600/600: AUROC=0.8863, AUPRC=0.8069\n",
      "Fold 6/10\n",
      "Epoch 600/600: AUROC=0.9081, AUPRC=0.8421\n",
      "Fold 7/10\n",
      "Epoch 600/600: AUROC=0.9225, AUPRC=0.8667\n",
      "Fold 8/10\n",
      "Epoch 600/600: AUROC=0.9363, AUPRC=0.8829\n",
      "Fold 9/10\n",
      "Epoch 600/600: AUROC=0.9493, AUPRC=0.9060\n",
      "Fold 10/10\n",
      "Epoch 600/600: AUROC=0.9572, AUPRC=0.9211\n",
      "Mean AUROC=0.8895, Mean AUPRC=0.8178\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "dataset = MyDataset(dynamic_features=torch.DoubleTensor(tensor_all), static_features=torch.DoubleTensor(static_feature), diff_cat_all=torch.FloatTensor(diff_cat_all), labels=torch.LongTensor(y_label_final))\n",
    "\n",
    "# model, loss, and optimizer\n",
    "model = MyModel()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 10-fold cross validation\n",
    "k = 10\n",
    "num_epochs = 600\n",
    "batch_size = 64\n",
    "k_fold_cross_validation(k, dataset, model, loss_fn, optimizer, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
